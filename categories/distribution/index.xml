<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distribution on 南乡清水</title>
    <link>https://nicky-chen.github.io/categories/distribution/</link>
    <description>Recent content in Distribution on 南乡清水</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>All rights reserved - 2018</copyright>
    <lastBuildDate>Mon, 15 Oct 2018 11:18:15 +0800</lastBuildDate>
    
	<atom:link href="https://nicky-chen.github.io/categories/distribution/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>分布式全局序列ID方案之Redis优化方案</title>
      <link>https://nicky-chen.github.io/2018/10/15/id-redis/</link>
      <pubDate>Mon, 15 Oct 2018 11:18:15 +0800</pubDate>
      
      <guid>https://nicky-chen.github.io/2018/10/15/id-redis/</guid>
      <description>1 Redis的Flicker方案 利用redis的lua脚本功能，在每个节点上通过lua脚本生成唯一ID，生成的ID为64位，具体如下：
使用41 bit来存放时间，精确到毫秒，可以使用到2039年 使用12 bit来存放逻辑分片ID，最大分片ID是4095 使用10 bit来存放自增长ID，则每个节点，每毫秒最多可生成1024个ID 比如GTM时间 2018年6月24日11点23分 ，它的距1970年的毫秒数是 1529810591000，假定分片ID是60，自增长序列是20，则生成的ID是：
6416490681073670164 = 1529810591000 &amp;lt;&amp;lt; 22 | 60 &amp;lt;&amp;lt; 10 | 20 redis提供了TIME命令，取得redis服务器的秒值和微秒值 毫秒值获取命令：EVAL &amp;quot;local current = redis.call(&#39;TIME&#39;) ;return a[1]*1000 + a[2]/1000&amp;quot; 0 生成最终ID : current &amp;lt;&amp;lt; (12 + 10)) | (shardingId &amp;lt;&amp;lt; 10) | seq  2 ID原子性自增方案 2.1 Redis HINCRBY 命令 Redis 的 INCR 命令支持 “INCR AND GET” 原子操作。利用这个特性，我们可以在 Redis 中存序列号，让分布式环境中多个取号服务在 Redis 中通过 INCR 命令来实现取号；同时 Redis 是单进程单线程架构，不会因为多个取号方的 INCR 命令导致取号重复。因此，基于 Redis 的 INCR 命令实现序列号的生成基本能满足全局唯一与单调递增的特性，并且性能还不错。</description>
    </item>
    
    <item>
      <title>分布式全局序列ID方案之Flicker优化方案</title>
      <link>https://nicky-chen.github.io/2018/09/25/id-flicker/</link>
      <pubDate>Tue, 25 Sep 2018 11:18:15 +0800</pubDate>
      
      <guid>https://nicky-chen.github.io/2018/09/25/id-flicker/</guid>
      <description>1 Flicker的解决方案 MySQL中id自增的特性，可以借此来生成全局的序列号，Flicker在解决全局ID生成方案里就采用了MySQL自增长ID的机制（auto_increment + replace into + MyISAM）。一个生成64位ID方案具体就是这样的： 先创建单独的数据库，然后创建一个表：
CREATE TABLE borrow_order ( id bigint(20) unsigned NOT NULL auto_increment, stub char(1) NOT NULL default &#39;&#39;, PRIMARY KEY (id), UNIQUE KEY stub (stub) ) ENGINE=MyISAM  当我们插入记录后，执行SELECT * from borrow_order ，查询结果就是这样的：
+-------------------+------+ | id | stub | +-------------------+------+ | 1 | 192.168.100.102 | +-------------------+------+  在我们的应用端需要做下面这两个操作，在一个事务会话里提交：
REPLACE INTO borrow_order (stub) VALUES (&#39;192.168.100.102&#39;); SELECT LAST_INSERT_ID();  上述操作，通过 replace into 操作，首先尝试插入数据到表中，如果发现表中已经有此行数据则先删除此行数据，然后插入新的数据。 如果没有此行数据的话，直接插入新数据。注意：插入的行字段需要有主键索引或者唯一索引，否则会出错</description>
    </item>
    
    <item>
      <title>分布式全局序列ID方案之Snowflake算法</title>
      <link>https://nicky-chen.github.io/2018/09/19/id-snowflake/</link>
      <pubDate>Wed, 19 Sep 2018 11:18:15 +0800</pubDate>
      
      <guid>https://nicky-chen.github.io/2018/09/19/id-snowflake/</guid>
      <description>#1 背景 在分布式项目中，在业务数据中需要生成一个全局唯一的序列号，比如：消息标识，订单标识，用户标识等等。同时对于id生成的要求如下： * 全局唯一 * 趋势有序 * 主键索引 方便排序 * 高可用 * 高并发
#2 基础方案
###2.1 数据库主键自增 利用mysql的auto_increment特性 &amp;gt;优点： （1）能够保证唯一性 （2）能够保证递增性 （3）步长固定
 缺点： （1）无法高可用：普通的一主多从+读写分离架构，自增ID写入请求，主库挂了就GG （2）无法高并发：写入是单点，数据库主库的写性能决定ID的生成性能上限，并且难以扩展
 ###2.2 UUID uuid算法是比较常用的算法，根据UUID的特性，可以产生一个唯一的字符串
 优点： （1）本地生成ID，无需远程服务调用，低延时 （2）扩展性好，基本可以-认为没有性能上限
缺点： （1）无法保证趋势递增 （2）uuid字符串过长，作为主键建立索引查询效率低，常见优化方案为“转化为两个uint64整数存储”或者“折半存储”（折半后不能保证唯一性） （3）如使用实现版本的=不一样，在高并发情况下可能会出现UUID重复情况
 ###2.3 时间戳
 优点： （1）本地生成ID，无需远程调用，低延时 （2）ID趋势递增 （3）ID是整数，建立索引后查询效率高
缺点： （1）如果并发量超过1000，会生成重复的ID
 #3 Twitter Snowflake
###3.1 简介 snowflake是twitter开源的分布式ID生成算法，其核心思想是：一个long型的ID，使用其中41bit作为毫秒数，10bit作为机器编号，12bit作为毫秒内序列号。这个算法单机每秒内理论上最多可以生成1000*(2^12)，也就是400W的ID，完全能满足业务的需求
###3.2 图示详解 （1）1位：标识部分，在java中由于long的最高位是符号位，正数是0，负数是1，一般生成的ID为正数，所以为0
（2）41位：时间戳部分，这个是毫秒级的时间，_一般实现上不会存储当前的时间戳，而是时间戳的差值（当前时间-固定的开始时间）_，这样可以使产生的ID从更小值开始；41位的时间戳可以使用69年，(1L &amp;lt;&amp;lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69年</description>
    </item>
    
    <item>
      <title>分布式项目知识要点</title>
      <link>https://nicky-chen.github.io/2018/06/13/distribution-knowledge/</link>
      <pubDate>Wed, 13 Jun 2018 11:18:15 +0800</pubDate>
      
      <guid>https://nicky-chen.github.io/2018/06/13/distribution-knowledge/</guid>
      <description></description>
    </item>
    
    <item>
      <title>分布式之【CAP理论、BASE理论 、FLP不可能定理】</title>
      <link>https://nicky-chen.github.io/2018/04/25/cap-base-flp/</link>
      <pubDate>Wed, 25 Apr 2018 11:18:15 +0800</pubDate>
      
      <guid>https://nicky-chen.github.io/2018/04/25/cap-base-flp/</guid>
      <description>1.分布式系统的CAP理论 1.1 CAP理论概述 2000年7月，加州大学伯克利分校的Eric Brewer教授在ACM PODC会议上提出CAP猜想。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。
一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。
1.2 CAP的定义 1.2.1 Consistency 一致性 一致性指“all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致。分布式的一致性
对于一致性，可以分为从客户端和服务端两个不同的视角。从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。
从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如果能容忍后续的部分或者全部访问不到，则是弱一致性。如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。
1.2.2 Availability 可用性 可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。
对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。也就是，该系统使用的任何算法必须最终终止。当同时要求分区容忍性时，这是一个很强的定义：即使是严重的网络错误，每个请求必须终止。
好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。可用性通常情况下可用性和分布式数据冗余，负载均衡等有着很大的关联。
1.2.3 Partition Tolerance分区容错性 分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。
分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。
1.3 CAP的证明 如上图，是我们证明CAP的基本场景，网络中有两个节点N1和N2，可以简单的理解N1和N2分别是两台计算机，他们之间网络可以连通，N1中有一个应用程序A，和一个数据库V，N2也有一个应用程序B2和一个数据库V。现在，A和B是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。
在满足一致性的时候，N1和N2中的数据是一样的，V0=V0。在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的正常运作。
如上图，是分布式系统正常运转的流程，用户向N1机器请求数据更新，程序A更新数据库Vo为V1，分布式系统将数据进行同步操作M，将V1同步的N2中V0，使得N2中的数据V0也更新为V1，N2中的数据再响应N2的请求。
这里，可以定义N1和N2的数据库V之间的数据是否一样为一致性；外部对N1和N2的请求响应为可用行；N1和N2之间的网络环境为分区容错性。这是正常运作的场景，也是理想的场景，然而现实是残酷的，当错误发生的时候，一致性和可用性还有分区容错性，是否能同时满足，还是说要进行取舍呢？
作为一个分布式系统，它和单机系统的最大区别，就在于网络，现在假设一种极端情况，N1和N2之间的网络断开了，我们要支持这种网络异常，相当于要满足分区容错性，能不能同时满足一致性和响应性呢？还是说要对他们进行取舍。
假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1，由于网络是断开的，所以分布式系统同步操作M，所以N2中的数据依旧是V0；这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据V1，怎么办呢？有二种选择，第一，牺牲数据一致性，响应旧的数据V0给用户；第二，牺牲可用性，阻塞等待，直到网络连接恢复，数据更新操作M完成之后，再给用户响应最新的数据V1。
这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。
1.4 CAP权衡 通过CAP理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？</description>
    </item>
    
    <item>
      <title>接口设计的幂等性考虑</title>
      <link>https://nicky-chen.github.io/2018/03/26/interface-idempotency/</link>
      <pubDate>Mon, 26 Mar 2018 11:18:15 +0800</pubDate>
      
      <guid>https://nicky-chen.github.io/2018/03/26/interface-idempotency/</guid>
      <description>分布式系统接口幂等性 1.幂等性定义 1.1 数学定义  在数学里，幂等有两种主要的定义：- 在某二元运算下，幂等元素是指被自己重复运算(或对于函数是为复合)的结果等于它自己的元素。例如，乘法下唯一两个幂等实数为0和1。即 s *s = s- 某一元运算为幂等的时，其作用在任一元素两次后会和其作用一次的结果相同。例如，高斯符号便是幂等的，即f(f(x)) = f(x)。
 1.2 HTTP规范的定义 在HTTP/1.1规范中幂等性的定义是：
 A request method is considered &amp;ldquo;idempotent&amp;rdquo; if the intended effect on the server of multiple identical requests with that method is the same as the effect for a single such request. Of the request methods defined by this specification, PUT, DELETE, and safe request methods are idempotent.
 HTTP的幂等性指的是一次和多次请求某一个资源应该具有相同的副作用。如通过PUT接口将数据的Status置为1，无论是第一次执行还是多次执行，获取到的结果应该是相同的，即执行完成之后Status =1。</description>
    </item>
    
  </channel>
</rss>